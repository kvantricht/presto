{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f47e98-297c-448d-b9ce-ad61bebec221",
   "metadata": {},
   "source": [
    "### First attempt to continue Presto pre-training on WorldCereal data\n",
    "\n",
    "Most code taken from https://github.com/nasaharvest/presto/blob/main/train.py, but only retained core parts to be able to test how Presto can eat the WorldCereal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f161c3be-3e4e-460e-a7e5-b8febf1b1e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, cast\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from presto import Presto\n",
    "from presto.dataops import BANDS_GROUPS_IDX, MASK_STRATEGIES, MaskParams\n",
    "\n",
    "\n",
    "from presto.model import LossWrapper, adjust_learning_rate, param_groups_weight_decay\n",
    "from presto.utils import (\n",
    "    DEFAULT_SEED,\n",
    "    config_dir,\n",
    "    device,\n",
    "    initialize_logging,\n",
    "    seed_everything,\n",
    "    timestamp_dirname,\n",
    "    update_data_dir,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"__main__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db440dbb-8b2b-41fc-81ee-3e3dcd47dff6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-10-2023 18:42:15 - INFO - Initialized logging to output/2023_10_25_18_42_15_093321/console-output.log\n",
      "25-10-2023 18:42:15 - INFO - Initialized logging to output/2023_10_25_18_42_15_093321/console-output.log\n",
      "25-10-2023 18:42:15 - INFO - Using output dir: output/2023_10_25_18_42_15_093321\n",
      "25-10-2023 18:42:15 - INFO - Using output dir: output/2023_10_25_18_42_15_093321\n"
     ]
    }
   ],
   "source": [
    "model_name = 'presto_worldcereal'\n",
    "seed = DEFAULT_SEED\n",
    "seed_everything(seed)\n",
    "output_parent_dir = Path('.')\n",
    "run_id = None\n",
    "\n",
    "logging_dir = output_parent_dir / \"output\" / timestamp_dirname(run_id)\n",
    "logging_dir.mkdir(exist_ok=True, parents=True)\n",
    "initialize_logging(logging_dir)\n",
    "logger.info(\"Using output dir: %s\" % logging_dir)\n",
    "\n",
    "# Taken the defaults for now\n",
    "num_epochs = 20\n",
    "val_per_n_steps = 1000\n",
    "dynamic_world_loss_weight = 2  # Set to 0 if we don't have DW?\n",
    "max_learning_rate = 0.0001  # 0.001 is default, for finetuning max should be lower?\n",
    "min_learning_rate = 0\n",
    "warmup_epochs = 2\n",
    "weight_decay = 0.05\n",
    "batch_size = 1  # default 4096\n",
    "\n",
    "# Default mask strategies and mask_ratio\n",
    "mask_strategies = MASK_STRATEGIES\n",
    "mask_ratio: float = 0.75\n",
    "\n",
    "path_to_config = config_dir / \"default.json\"\n",
    "model_kwargs = json.load(Path(path_to_config).open(\"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db61b07-2788-44f7-91f5-53f7afa1675e",
   "metadata": {},
   "source": [
    "### Load a (very) small test dataframe from WorldCereal\n",
    "\n",
    "It's been reprocessed to match better Presto requirements, most notably:\n",
    "- monthly instead of 10-day compositing\n",
    "- outputing as 'features' the composited time series of all required bands instead of expert features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c58bc99f-3ac8-4a31-8e1c-a5ae4ffeedbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OPTICAL-B02-ts0-10m</th>\n",
       "      <th>OPTICAL-B02-ts1-10m</th>\n",
       "      <th>OPTICAL-B02-ts2-10m</th>\n",
       "      <th>OPTICAL-B02-ts3-10m</th>\n",
       "      <th>OPTICAL-B02-ts4-10m</th>\n",
       "      <th>OPTICAL-B02-ts5-10m</th>\n",
       "      <th>OPTICAL-B02-ts6-10m</th>\n",
       "      <th>OPTICAL-B02-ts7-10m</th>\n",
       "      <th>OPTICAL-B02-ts8-10m</th>\n",
       "      <th>OPTICAL-B02-ts9-10m</th>\n",
       "      <th>...</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>CT</th>\n",
       "      <th>OUTPUT</th>\n",
       "      <th>IRR</th>\n",
       "      <th>location_id</th>\n",
       "      <th>ref_id</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>aez_zoneid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>468</td>\n",
       "      <td>0</td>\n",
       "      <td>581</td>\n",
       "      <td>534</td>\n",
       "      <td>670</td>\n",
       "      <td>381</td>\n",
       "      <td>329</td>\n",
       "      <td>458</td>\n",
       "      <td>653</td>\n",
       "      <td>1261</td>\n",
       "      <td>...</td>\n",
       "      <td>50.897915</td>\n",
       "      <td>2.668585</td>\n",
       "      <td>1110</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0000280664EDE418</td>\n",
       "      <td>2018_BE_LPIS-Flanders</td>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>46172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>741</td>\n",
       "      <td>0</td>\n",
       "      <td>267</td>\n",
       "      <td>354</td>\n",
       "      <td>567</td>\n",
       "      <td>275</td>\n",
       "      <td>249</td>\n",
       "      <td>425</td>\n",
       "      <td>712</td>\n",
       "      <td>958</td>\n",
       "      <td>...</td>\n",
       "      <td>50.897903</td>\n",
       "      <td>2.664141</td>\n",
       "      <td>1510</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0000280664EDE418</td>\n",
       "      <td>2018_BE_LPIS-Flanders</td>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>46172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>359</td>\n",
       "      <td>5651</td>\n",
       "      <td>339</td>\n",
       "      <td>436</td>\n",
       "      <td>609</td>\n",
       "      <td>289</td>\n",
       "      <td>389</td>\n",
       "      <td>509</td>\n",
       "      <td>663</td>\n",
       "      <td>1100</td>\n",
       "      <td>...</td>\n",
       "      <td>50.893848</td>\n",
       "      <td>2.665414</td>\n",
       "      <td>1110</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0000280664EDE418</td>\n",
       "      <td>2018_BE_LPIS-Flanders</td>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>46172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>510</td>\n",
       "      <td>7468</td>\n",
       "      <td>518</td>\n",
       "      <td>516</td>\n",
       "      <td>781</td>\n",
       "      <td>1158</td>\n",
       "      <td>1008</td>\n",
       "      <td>1236</td>\n",
       "      <td>540</td>\n",
       "      <td>456</td>\n",
       "      <td>...</td>\n",
       "      <td>50.892136</td>\n",
       "      <td>2.660255</td>\n",
       "      <td>9520</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0000280664EDE418</td>\n",
       "      <td>2018_BE_LPIS-Flanders</td>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>46172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358</td>\n",
       "      <td>6283</td>\n",
       "      <td>389</td>\n",
       "      <td>453</td>\n",
       "      <td>719</td>\n",
       "      <td>1141</td>\n",
       "      <td>1141</td>\n",
       "      <td>758</td>\n",
       "      <td>587</td>\n",
       "      <td>484</td>\n",
       "      <td>...</td>\n",
       "      <td>50.892136</td>\n",
       "      <td>2.660255</td>\n",
       "      <td>9520</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0000280664EDE418</td>\n",
       "      <td>2018_BE_LPIS-Flanders</td>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>46172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OPTICAL-B02-ts0-10m  OPTICAL-B02-ts1-10m  OPTICAL-B02-ts2-10m  \\\n",
       "0                  468                    0                  581   \n",
       "1                  741                    0                  267   \n",
       "2                  359                 5651                  339   \n",
       "3                  510                 7468                  518   \n",
       "4                  358                 6283                  389   \n",
       "\n",
       "   OPTICAL-B02-ts3-10m  OPTICAL-B02-ts4-10m  OPTICAL-B02-ts5-10m  \\\n",
       "0                  534                  670                  381   \n",
       "1                  354                  567                  275   \n",
       "2                  436                  609                  289   \n",
       "3                  516                  781                 1158   \n",
       "4                  453                  719                 1141   \n",
       "\n",
       "   OPTICAL-B02-ts6-10m  OPTICAL-B02-ts7-10m  OPTICAL-B02-ts8-10m  \\\n",
       "0                  329                  458                  653   \n",
       "1                  249                  425                  712   \n",
       "2                  389                  509                  663   \n",
       "3                 1008                 1236                  540   \n",
       "4                 1141                  758                  587   \n",
       "\n",
       "   OPTICAL-B02-ts9-10m  ...        lat       lon    CT  OUTPUT  IRR  \\\n",
       "0                 1261  ...  50.897915  2.668585  1110      11    0   \n",
       "1                  958  ...  50.897903  2.664141  1510      11    0   \n",
       "2                 1100  ...  50.893848  2.665414  1110      11    0   \n",
       "3                  456  ...  50.892136  2.660255  9520      12    0   \n",
       "4                  484  ...  50.892136  2.660255  9520      12    0   \n",
       "\n",
       "        location_id                 ref_id  start_date    end_date  aez_zoneid  \n",
       "0  0000280664EDE418  2018_BE_LPIS-Flanders  2017-10-27  2018-10-26       46172  \n",
       "1  0000280664EDE418  2018_BE_LPIS-Flanders  2017-10-27  2018-10-26       46172  \n",
       "2  0000280664EDE418  2018_BE_LPIS-Flanders  2017-10-27  2018-10-26       46172  \n",
       "3  0000280664EDE418  2018_BE_LPIS-Flanders  2017-10-27  2018-10-26       46172  \n",
       "4  0000280664EDE418  2018_BE_LPIS-Flanders  2017-10-27  2018-10-26       46172  \n",
       "\n",
       "[5 rows x 183 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('worldcereal_testdf.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342bd0f-4b40-4c8f-ad50-5c594c148f5a",
   "metadata": {},
   "source": [
    "### Making WorldCereal data compatible for Presto\n",
    "\n",
    "Uses a `WorldCerealDataset` pytorch dataset that upon requesting an item performs the required conversions to Presto inputs.\n",
    "If `mask_params` is provided, inputs will get the shape of what Presto pretraining normally requires, including a generated `mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69fbccfe-8434-42d5-8f4f-086eca85267c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-10-2023 18:42:18 - INFO - Setting up dataloaders\n",
      "25-10-2023 18:42:18 - INFO - Setting up dataloaders\n"
     ]
    }
   ],
   "source": [
    "from ewoc_presto import WorldCerealDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "logger.info(\"Setting up dataloaders\")\n",
    "\n",
    "# Load the mask parameters\n",
    "mask_params = MaskParams(mask_strategies, mask_ratio)\n",
    "\n",
    "# Create the WorldCereal dataset\n",
    "ds = WorldCerealDataset(df, mask_params=mask_params)\n",
    "\n",
    "# Create DataLoaders from the dataset. For now, without shame using same data for train and val\n",
    "# we're just testing functionality ;-)\n",
    "train_dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a57b43-bb29-4f38-88b2-486dcbce52f2",
   "metadata": {},
   "source": [
    "Check what an item from this dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e7dc72-5b74-4b75-a3fa-05e28ef541e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedExample(mask_eo=array([[ True,  True,  True,  True,  True, False, False, False,  True,\n",
       "         True, False, False,  True,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True, False, False, False,  True,\n",
       "         True, False, False,  True,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True, False, False, False,  True,\n",
       "         True, False, False,  True,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True, False, False, False,  True,\n",
       "         True, False, False,  True,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True, False, False, False,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True, False, False, False,  True,\n",
       "         True, False, False,  True,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False, False,  True,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True, False, False, False,  True,\n",
       "         True, False, False,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True, False, False, False,  True,\n",
       "         True, False, False,  True,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False],\n",
       "       [ True,  True,  True,  True,  True, False, False, False,  True,\n",
       "         True, False, False,  True,  True,  True,  True,  True]]), mask_dw=array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True]), x_eo=array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1019    , 0.1503    , 0.166     , 0.        , 0.        ,\n",
       "        0.1535    , 0.0989    , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.3449576 ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1346    , 0.1954    , 0.2099    , 0.        , 0.        ,\n",
       "        0.1832    , 0.1263    , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.3669243 ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1353    , 0.2045    , 0.2241    , 0.        , 0.        ,\n",
       "        0.1706    , 0.1067    , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.40958264],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.39922732],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1041    , 0.2315    , 0.284     , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.6640794 ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0976    , 0.2935    , 0.3797    , 0.        , 0.        ,\n",
       "        0.1708    , 0.0986    , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.794592  ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1556    , 0.0773    , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.81214   ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1725    , 0.2673    , 0.3199    , 0.        , 0.        ,\n",
       "        0.2067    , 0.1265    , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.2005    , 0.257     , 0.279     , 0.        , 0.        ,\n",
       "        0.3017    , 0.2286    , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.25565   ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.31101024],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.1292    , 0.1477    , 0.1639    , 0.        , 0.        ,\n",
       "        0.2095    , 0.1863    , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]], dtype=float32), y_eo=array([[7.1070445e-01, 4.0414044e-01, 4.6799999e-02, 6.3299999e-02,\n",
       "        6.9499999e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.4270000e-01, 1.7900001e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "        2.8600028e-01, 1.7580000e+00, 8.0000004e-03, 2.2097086e-04,\n",
       "        0.0000000e+00],\n",
       "       [7.6310456e-01, 4.4350997e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.6571482e-01, 3.1013334e+00, 8.0000004e-03, 2.2097086e-04,\n",
       "        0.0000000e+00],\n",
       "       [7.7439803e-01, 4.4943887e-01, 5.8100000e-02, 8.9000002e-02,\n",
       "        1.0240000e-01, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        2.2110000e-01, 2.2900000e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.9371425e-01, 3.5703335e+00, 8.0000004e-03, 2.2097086e-04,\n",
       "        0.0000000e+00],\n",
       "       [7.4385452e-01, 4.0304527e-01, 5.3399999e-02, 8.3099999e-02,\n",
       "        9.5500000e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        2.2800000e-01, 2.4140000e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.2000035e-01, 1.4010000e+00, 8.0000004e-03, 2.2097086e-04,\n",
       "        0.0000000e+00],\n",
       "       [7.1858895e-01, 3.6650026e-01, 6.7000002e-02, 8.9900002e-02,\n",
       "        9.3300000e-02, 1.3800000e-01, 2.0850000e-01, 2.2849999e-01,\n",
       "        2.1730000e-01, 2.4770001e-01, 1.9509999e-01, 1.3169999e-01,\n",
       "        1.6885725e-01, 2.0913334e+00, 8.0000004e-03, 2.2097086e-04,\n",
       "        0.0000000e+00],\n",
       "       [6.9431806e-01, 3.9765644e-01, 3.8100000e-02, 6.5099999e-02,\n",
       "        5.4099999e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        2.6800001e-01, 2.9820001e-01, 1.6370000e-01, 9.5500000e-02,\n",
       "        3.3199987e-01, 2.6080000e+00, 8.0000004e-03, 2.2097086e-04,\n",
       "        0.0000000e+00],\n",
       "       [4.8248756e-01, 3.1345016e-01, 3.2900002e-02, 6.3400000e-02,\n",
       "        4.3299999e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        3.7830001e-01, 4.0000001e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "        3.5657173e-01, 2.7900002e+00, 8.0000004e-03, 2.2097086e-04,\n",
       "        0.0000000e+00],\n",
       "       [5.0303054e-01, 3.1309509e-01, 4.5800000e-02, 6.2300000e-02,\n",
       "        4.2399999e-02, 8.6999997e-02, 2.9620001e-01, 4.1389999e-01,\n",
       "        4.0900001e-01, 4.4100001e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "        4.9171403e-01, 8.6833334e-01, 8.0000004e-03, 2.2097086e-04,\n",
       "        0.0000000e+00],\n",
       "       [6.1433578e-01, 3.7268978e-01, 6.5300003e-02, 1.0580000e-01,\n",
       "        1.1890000e-01, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        3.3680001e-01, 3.6379999e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "        5.7457191e-01, 1.8366668e-01, 8.0000004e-03, 2.2097086e-04,\n",
       "        4.7816548e-01],\n",
       "       [6.6410881e-01, 3.2872450e-01, 1.2610000e-01, 1.5770000e-01,\n",
       "        1.7950000e-01, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        3.0280000e-01, 3.0300000e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "        5.9200001e-01, 2.0303335e+00, 8.0000004e-03, 2.2097086e-04,\n",
       "        0.0000000e+00],\n",
       "       [5.9743655e-01, 2.8575873e-01, 7.5400002e-02, 1.0820000e-01,\n",
       "        1.2140000e-01, 1.5680000e-01, 2.0370001e-01, 2.2139999e-01,\n",
       "        2.3100001e-01, 2.3649999e-01, 2.4150001e-01, 2.0400000e-01,\n",
       "        4.8828560e-01, 2.1930001e+00, 8.0000004e-03, 2.2097086e-04,\n",
       "        0.0000000e+00],\n",
       "       [5.5043799e-01, 2.3649834e-01, 5.7000000e-02, 8.2699999e-02,\n",
       "        9.9299997e-02, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.6380000e-01, 1.8080001e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "        4.0114310e-01, 1.3730000e+00, 8.0000004e-03, 2.2097086e-04,\n",
       "        2.4515395e-01]], dtype=float32), x_dw=array([9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.], dtype=float32), y_dw=array([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9], dtype=int16), start_month=tensor(9), latlon=tensor([50.8979,  2.6686]), strategy='group_bands')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad68850d-5c0d-4594-b4c6-0478b8e15163",
   "metadata": {},
   "source": [
    "### Setup Presto model and load pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1793713-09a7-4dd0-a7c0-30a7f4dd4d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25-10-2023 18:42:24 - INFO - Setting up model\n",
      "25-10-2023 18:42:24 - INFO - Setting up model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Presto(\n",
       "  (encoder): Encoder(\n",
       "    (eo_patch_embed): ModuleDict(\n",
       "      (S1): Linear(in_features=2, out_features=128, bias=True)\n",
       "      (S2_RGB): Linear(in_features=3, out_features=128, bias=True)\n",
       "      (S2_Red_Edge): Linear(in_features=3, out_features=128, bias=True)\n",
       "      (S2_NIR_10m): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (S2_NIR_20m): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (S2_SWIR): Linear(in_features=2, out_features=128, bias=True)\n",
       "      (ERA5): Linear(in_features=2, out_features=128, bias=True)\n",
       "      (SRTM): Linear(in_features=2, out_features=128, bias=True)\n",
       "      (NDVI): Linear(in_features=1, out_features=128, bias=True)\n",
       "    )\n",
       "    (dw_embed): Embedding(10, 128)\n",
       "    (latlon_embed): Linear(in_features=3, out_features=128, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (month_embed): Embedding(12, 32)\n",
       "    (channel_embed): Embedding(10, 32)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_embed): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0-1): 2 x Block(\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (eo_decoder_pred): ModuleDict(\n",
       "      (S1): Linear(in_features=128, out_features=2, bias=True)\n",
       "      (S2_RGB): Linear(in_features=128, out_features=3, bias=True)\n",
       "      (S2_Red_Edge): Linear(in_features=128, out_features=3, bias=True)\n",
       "      (S2_NIR_10m): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (S2_NIR_20m): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (S2_SWIR): Linear(in_features=128, out_features=2, bias=True)\n",
       "      (ERA5): Linear(in_features=128, out_features=2, bias=True)\n",
       "      (SRTM): Linear(in_features=128, out_features=2, bias=True)\n",
       "      (NDVI): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "    (dw_decoder_pred): Linear(in_features=128, out_features=9, bias=True)\n",
       "    (channel_embeddings): Embedding(10, 32)\n",
       "    (month_embed): Embedding(12, 48)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(\"Setting up model\")\n",
    "model = Presto.load_pretrained()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "814fc9c4-6eb0-4a19-be5e-796e56083563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters: keep unchanged for now\n",
    "param_groups = param_groups_weight_decay(model, weight_decay)\n",
    "optimizer = optim.AdamW(param_groups, lr=max_learning_rate, betas=(0.9, 0.95))\n",
    "mse = LossWrapper(nn.MSELoss())\n",
    "ce = LossWrapper(nn.CrossEntropyLoss())\n",
    "\n",
    "training_config = {\n",
    "    \"model\": model.__class__,\n",
    "    \"encoder\": model.encoder.__class__,\n",
    "    \"decoder\": model.decoder.__class__,\n",
    "    \"optimizer\": optimizer.__class__.__name__,\n",
    "    \"eo_loss\": mse.loss.__class__.__name__,\n",
    "    \"dynamic_world_loss\": ce.loss.__class__.__name__,\n",
    "    \"device\": device,\n",
    "    \"logging_dir\": logging_dir,\n",
    "    # **args,\n",
    "    # **model_kwargs,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c26b1-a10b-4101-909e-65edaaebfdc2",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Copy from original code the relevant parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddfada35-31de-4e99-b2fb-014ba12eb5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Train:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch:   0%|          | 0/20 [00:01<?, ?it/s] \u001b[A\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BANDS_GROUPS_IDX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m\n\u001b[1;32m     33\u001b[0m y_pred, dw_pred \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     34\u001b[0m     x, mask\u001b[38;5;241m=\u001b[39mmask, dynamic_world\u001b[38;5;241m=\u001b[39mx_dw, latlons\u001b[38;5;241m=\u001b[39mlatlons, month\u001b[38;5;241m=\u001b[39mstart_month\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# set all SRTM timesteps except the first one to unmasked, so that\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# they will get ignored by the loss function even if the SRTM\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# value was masked\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m mask[:, \u001b[38;5;241m1\u001b[39m:, \u001b[43mBANDS_GROUPS_IDX\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSRTM\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse(y_pred[mask], y[mask])\n\u001b[1;32m     41\u001b[0m dw_loss \u001b[38;5;241m=\u001b[39m ce(dw_pred[dw_mask], y_dw[dw_mask])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BANDS_GROUPS_IDX' is not defined"
     ]
    }
   ],
   "source": [
    "lowest_validation_loss = None\n",
    "best_val_epoch = 0\n",
    "training_step = 0\n",
    "num_validations = 0\n",
    "dataloader_length = df.shape[0]\n",
    "\n",
    "with tqdm(range(num_epochs), desc=\"Epoch\") as tqdm_epoch:\n",
    "    for epoch in tqdm_epoch:\n",
    "        # ------------------------ Training ----------------------------------------\n",
    "        total_train_loss = 0.0\n",
    "        total_eo_train_loss = 0.0\n",
    "        total_dw_train_loss = 0.0\n",
    "        total_num_eo_values_masked = 0\n",
    "        total_num_dw_values_masked = 0\n",
    "        num_updates_being_captured = 0\n",
    "        train_size = 0\n",
    "        model.train()\n",
    "        for epoch_step, b in enumerate(tqdm(train_dataloader, desc=\"Train\", leave=False)):\n",
    "            mask, x, y, start_month = b[0].to(device), b[2].to(device), b[3].to(device), b[6]\n",
    "            dw_mask, x_dw, y_dw = b[1].to(device), b[4].to(device).long(), b[5].to(device).long()\n",
    "            latlons = b[7].to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            lr = adjust_learning_rate(\n",
    "                optimizer,\n",
    "                epoch_step / dataloader_length + epoch,\n",
    "                warmup_epochs,\n",
    "                num_epochs,\n",
    "                max_learning_rate,\n",
    "                min_learning_rate,\n",
    "            )\n",
    "            # Get model outputs and calculate loss\n",
    "            y_pred, dw_pred = model(\n",
    "                x, mask=mask, dynamic_world=x_dw, latlons=latlons, month=start_month\n",
    "            )\n",
    "            # set all SRTM timesteps except the first one to unmasked, so that\n",
    "            # they will get ignored by the loss function even if the SRTM\n",
    "            # value was masked\n",
    "            mask[:, 1:, BANDS_GROUPS_IDX[\"SRTM\"]] = False\n",
    "            loss = mse(y_pred[mask], y[mask])\n",
    "            dw_loss = ce(dw_pred[dw_mask], y_dw[dw_mask])\n",
    "            num_eo_masked, num_dw_masked = len(y_pred[mask]), len(dw_pred[dw_mask])\n",
    "            with torch.no_grad():\n",
    "                ratio = num_dw_masked / max(num_eo_masked, 1)\n",
    "                # weight shouldn't be > 1\n",
    "                weight = min(1, dynamic_world_loss_weight * ratio)\n",
    "\n",
    "            total_loss = loss + weight * dw_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            current_batch_size = len(x)\n",
    "            total_train_loss += total_loss.item()\n",
    "            total_eo_train_loss += loss.item() * num_eo_masked\n",
    "            total_dw_train_loss += dw_loss.item() * num_dw_masked\n",
    "            total_num_eo_values_masked += num_eo_masked\n",
    "            total_num_dw_values_masked += num_dw_masked\n",
    "            num_updates_being_captured += 1\n",
    "            train_size += current_batch_size\n",
    "            training_step += 1\n",
    "\n",
    "            # ------------------------ Validation --------------------------------------\n",
    "            if training_step % val_per_n_steps == 0:\n",
    "                total_val_loss = 0.0\n",
    "                total_eo_val_loss = 0.0\n",
    "                total_dw_val_loss = 0.0\n",
    "                total_val_num_eo_values_masked = 0\n",
    "                total_val_num_dw_values_masked = 0\n",
    "                num_val_updates_captured = 0\n",
    "                val_size = 0\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for b in tqdm(val_dataloader, desc=\"Validate\"):\n",
    "                        mask, x, y, start_month = (\n",
    "                            b[0].to(device),\n",
    "                            b[2].to(device),\n",
    "                            b[3].to(device),\n",
    "                            b[6],\n",
    "                        )\n",
    "                        dw_mask, x_dw = b[1].to(device), b[4].to(device).long()\n",
    "                        y_dw, latlons = b[5].to(device).long(), b[7].to(device)\n",
    "                        # Get model outputs and calculate loss\n",
    "                        y_pred, dw_pred = model(\n",
    "                            x, mask=mask, dynamic_world=x_dw, latlons=latlons, month=start_month\n",
    "                        )\n",
    "                        # set all SRTM timesteps except the first one to unmasked, so that\n",
    "                        # they will get ignored by the loss function even if the SRTM\n",
    "                        # value was masked\n",
    "                        mask[:, 1:, BANDS_GROUPS_IDX[\"SRTM\"]] = False\n",
    "                        loss = mse(y_pred[mask], y[mask])\n",
    "                        dw_loss = ce(dw_pred[dw_mask], y_dw[dw_mask])\n",
    "                        num_eo_masked, num_dw_masked = len(y_pred[mask]), len(dw_pred[dw_mask])\n",
    "                        with torch.no_grad():\n",
    "                            ratio = num_dw_masked / max(num_eo_masked, 1)\n",
    "                            # weight shouldn't be > 1\n",
    "                            weight = min(1, dynamic_world_loss_weight * ratio)\n",
    "                        total_loss = loss + weight * dw_loss\n",
    "                        current_batch_size = len(x)\n",
    "                        val_size += current_batch_size\n",
    "                        total_val_loss += total_loss.item()\n",
    "                        total_eo_val_loss += loss.item() * num_eo_masked\n",
    "                        total_dw_val_loss += dw_loss.item() * num_dw_masked\n",
    "                        total_val_num_eo_values_masked += num_eo_masked\n",
    "                        total_val_num_dw_values_masked += num_dw_masked\n",
    "                        num_val_updates_captured += 1\n",
    "\n",
    "                # ------------------------ Metrics + Logging -------------------------------\n",
    "                # train_loss now reflects the value against which we calculate gradients\n",
    "                train_loss = total_train_loss / num_updates_being_captured\n",
    "                train_eo_loss = total_eo_train_loss / max(total_num_eo_values_masked, 1)\n",
    "                train_dw_loss = total_dw_train_loss / max(total_num_dw_values_masked, 1)\n",
    "\n",
    "                val_loss = total_val_loss / num_val_updates_captured\n",
    "                val_eo_loss = total_eo_val_loss / max(total_val_num_eo_values_masked, 1)\n",
    "                val_dw_loss = total_dw_val_loss / max(total_val_num_dw_values_masked, 1)\n",
    "\n",
    "                if \"train_size\" not in training_config and \"val_size\" not in training_config:\n",
    "                    training_config[\"train_size\"] = train_size\n",
    "                    training_config[\"val_size\"] = val_size\n",
    "                    if wandb_enabled:\n",
    "                        wandb.config.update(training_config)\n",
    "\n",
    "                to_log = {\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"train_eo_loss\": train_eo_loss,\n",
    "                    \"val_eo_loss\": val_eo_loss,\n",
    "                    \"train_dynamic_world_loss\": train_dw_loss,\n",
    "                    \"val_dynamic_world_loss\": val_dw_loss,\n",
    "                    \"training_step\": training_step,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"lr\": lr,\n",
    "                }\n",
    "                tqdm_epoch.set_postfix(loss=val_loss)\n",
    "\n",
    "                if lowest_validation_loss is None or val_loss < lowest_validation_loss:\n",
    "                    lowest_validation_loss = val_loss\n",
    "                    best_val_epoch = epoch\n",
    "\n",
    "                    model_path = logging_dir / Path(\"models\")\n",
    "                    model_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "                    best_model_path = model_path / f\"{model_name}{epoch}.pt\"\n",
    "                    logger.info(f\"Saving best model to: {best_model_path}\")\n",
    "                    torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "                # reset training logging\n",
    "                total_train_loss = 0.0\n",
    "                total_eo_train_loss = 0.0\n",
    "                total_dw_train_loss = 0.0\n",
    "                total_num_eo_values_masked = 0\n",
    "                total_num_dw_values_masked = 0\n",
    "                num_updates_being_captured = 0\n",
    "                train_size = 0\n",
    "                num_validations += 1\n",
    "\n",
    "                # if wandb_enabled:\n",
    "                #     model.eval()\n",
    "                #     for title, plot in plot_predictions(model):\n",
    "                #         to_log[title] = plot\n",
    "                #     wandb.log(to_log)\n",
    "                #     plt.close(\"all\")\n",
    "\n",
    "                model.train()\n",
    "\n",
    "logger.info(f\"Done training, best model saved to {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598452c3-59d9-44b5-9373-0647cb4fd5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "worldcereal",
   "language": "python",
   "name": "worldcereal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
